#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
AdGuardHome æ‰¹é‡æ¸…ç†è„šæœ¬
---------------------------------
åŠŸèƒ½ï¼š
1ï¸âƒ£ è¯»å– urls.txt ä¸­çš„æ‰€æœ‰æº
2ï¸âƒ£ ä¸‹è½½æ¯ä¸ªæºçš„è§„åˆ™æ–‡ä»¶
3ï¸âƒ£ åˆ é™¤ AdGuardHome æ— æ³•è¯†åˆ«æˆ–æ— æ•ˆè§„åˆ™
4ï¸âƒ£ æ£€æµ‹åŸŸåæ˜¯å¦å¯è§£æï¼Œæ¸…ç†å¤±æ•ˆè§„åˆ™
5ï¸âƒ£ å»é™¤å­åŸŸé‡å¤ï¼Œä»…ä¿ç•™çˆ¶åŸŸ
6ï¸âƒ£ åˆå¹¶æ‰€æœ‰æœ‰æ•ˆè§„åˆ™ï¼Œç”Ÿæˆæ€» blocklist
7ï¸âƒ£ ä¿å­˜æ¯ä¸ªæºçš„åˆ é™¤æ—¥å¿—

ä½œè€…ï¼šwxglenovo
"""

import dns.resolver
import concurrent.futures
import sys
import re
import os
import requests
from urllib.parse import urlparse

INPUT_FILE = 'urls.txt'              # urls.txt æ–‡ä»¶
OUTPUT_FILE = 'blocklist_valid_merged.txt'  # åˆå¹¶åçš„æœ‰æ•ˆè§„åˆ™
MAX_WORKERS = 20                     # å¹¶è¡Œçº¿ç¨‹æ•°

resolver = dns.resolver.Resolver()
resolver.lifetime = 5
resolver.timeout = 5

checked_domains = {}  # ç¼“å­˜åŸŸåè§£æç»“æœ

# ----------------------------
# å·¥å…·å‡½æ•°
# ----------------------------
def is_valid_domain(domain: str) -> bool:
    if not domain or len(domain) < 4:
        return False
    if domain in checked_domains:
        return checked_domains[domain]
    try:
        resolver.resolve(domain, 'A')
        checked_domains[domain] = True
        return True
    except Exception:
        checked_domains[domain] = False
        return False


def clean_domain(line: str) -> str:
    domain = line.lstrip('|').lstrip('.')
    domain = domain.split('^')[0]
    domain = re.sub(r'[^a-zA-Z0-9\.\-]', '', domain)
    return domain


def get_parent_domain(domain: str) -> str:
    parts = domain.split('.')
    if len(parts) >= 2:
        return '.'.join(parts[-2:])
    return domain


def is_useless_rule(line: str) -> bool:
    invalid_patterns = [
        r'\$\$', r'##', r'#@#', r'#\?#',
        r'\$removeparam=', r'\$redirect=', r'\$rewrite=',
        r'\$domain=', r'\$third-party',
        r'/[a-zA-Z0-9_\-]+(\.js|\.css|\.png|\.jpg|\.gif|\.svg|\.json)',
        r'\*'
    ]
    return any(re.search(p, line) for p in invalid_patterns)


def check_rule(line: str) -> str:
    line = line.strip()
    if not line or line.startswith('#'):
        return line
    if is_useless_rule(line):
        return None
    if line.startswith(('||', '|', '.')):
        if '*' in line or '$script' in line or '/' in line:
            return line
        domain = clean_domain(line)
        if is_valid_domain(domain):
            return line
        else:
            return None
    else:
        return line


def remove_subdomain_conflicts(rules):
    domain_map = {}
    final_rules = []
    deleted = []

    for line in rules:
        line_strip = line.strip()
        if not line_strip or line_strip.startswith('#'):
            final_rules.append(line_strip)
            continue
        if line_strip.startswith(('||', '|', '.')):
            domain = clean_domain(line_strip)
            parent = get_parent_domain(domain)
            if parent in domain_map:
                deleted.append(line_strip)
                continue
            else:
                domain_map[parent] = line_strip
                final_rules.append(line_strip)
        else:
            final_rules.append(line_strip)
    return final_rules, deleted


def process_url(url: str):
    """ä¸‹è½½æºæ–‡ä»¶å¹¶æ¸…ç†"""
    try:
        r = requests.get(url, timeout=15)
        r.raise_for_status()
        lines = r.text.splitlines()
        print(f"ğŸ“¦ ä¸‹è½½ {url} å…± {len(lines)} æ¡è§„åˆ™")
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url}: {e}")
        return [], []

    # å¹¶è¡Œæ£€æµ‹è§„åˆ™
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        results = list(executor.map(check_rule, lines))

    valid_rules = [r for r in results if r is not None]
    deleted_invalid = [lines[i].strip() for i, r in enumerate(results) if r is None]

    final_rules, deleted_subdomain = remove_subdomain_conflicts(valid_rules)
    deleted_rules = deleted_invalid + deleted_subdomain

    # ä¿å­˜å•ä¸ªæºçš„æ—¥å¿—
    safe_name = re.sub(r'[^a-zA-Z0-9]', '_', url)
    log_file = f"deleted_{safe_name}.log"
    with open(log_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join([r for r in deleted_rules if r.strip()]))

    print(f"âœ… æ¸…ç†å®Œæˆ {url} -> æœ‰æ•ˆè§„åˆ™ {len(final_rules)}ï¼Œåˆ é™¤ {len(deleted_rules)}")
    return final_rules, log_file


def main():
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {INPUT_FILE}")
        sys.exit(1)

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        urls = [line.strip() for line in f if line.strip()]

    all_rules = []
    logs = []

    for url in urls:
        rules, log_file = process_url(url)
        all_rules.extend(rules)
        logs.append(log_file)

    # åˆå¹¶å»é‡
    merged_rules, _ = remove_subdomain_conflicts(all_rules)

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write('\n'.join([r for r in merged_rules if r.strip()]))

    print(f"ğŸ‰ æ‰€æœ‰æºæ¸…ç†åˆå¹¶å®Œæˆ -> {OUTPUT_FILE} å…± {len(merged_rules)} æ¡è§„åˆ™")
    print("æ—¥å¿—æ–‡ä»¶:", ', '.join(logs))


if __name__ == '__main__':
    main()
