name: Build AdGuardHome rules

on:
  schedule:
    - cron: "0 0 * * *"
    - cron: "0 6 * * *"
    - cron: "0 12 * * *"
    - cron: "0 18 * * *"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download & Merge Rules (Clean + Deduplicate)
        run: |
          mkdir -p dist tmp

          urls=(
            "https://raw.githubusercontent.com/wxglenovo/AdGuardHome-Filter/refs/heads/main/Create%20a%20Custom%20AdGuard%20Home%20Filtering%20Rule.txt"
            "https://raw.githubusercontent.com/BlueSkyXN/AdGuardHomeRules/master/skyrules.txt"
            "https://easylist-downloads.adblockplus.org/easylist.txt"
            "https://easylist-downloads.adblockplus.org/easylistchina.txt"
            "https://easylist-downloads.adblockplus.org/easyprivacy.txt"
            "https://raw.githubusercontent.com/banbendalao/ADgk/master/ADgk.txt"
            "https://raw.githubusercontent.com/banbendalao/ADgk/master/kill-baidu-ad.txt"
            "https://raw.githubusercontent.com/217heidai/adblockfilters/main/rules/adblockdnslite.txt"
            "https://raw.githubusercontent.com/REIJI007/Adblock-Rule-Collection/main/ADBLOCK_RULE_COLLECTION_DNS.txt"
            "https://anti-ad.net/easylist.txt"
            "https://gh-proxy.com/raw.githubusercontent.com/changzhaoCZ/fqnovel-adrules/refs/heads/main/fqnovel-fxxk_ads"
            "https://www.i-dont-care-about-cookies.eu/abp/"
          )

          echo "📥 下载所有规则..."
          for url in "${urls[@]}"; do
            echo "Downloading $url"
            curl -sSL --retry 3 --fail "$url" >> tmp/all_raw.txt || echo "Failed $url"
            echo "" >> tmp/all_raw.txt
          done

          echo "🧹 清理注释和空行..."
          grep -vE '^[[:space:]]*[!#]' tmp/all_raw.txt | sed '/^[[:space:]]*$/d' > tmp/all_cleaned.txt

          echo "📊 去重规则..."
          sort tmp/all_cleaned.txt | uniq > tmp/all_deduped_tmp.txt

          # ✅ 分离白名单和阻止规则
          grep '^@@||' tmp/all_deduped_tmp.txt > tmp/whitelist.txt
          grep -v '^@@||' tmp/all_deduped_tmp.txt > tmp/blocklist.txt

          # ✅ wxglenovo 白名单优先
          curl -sSL --retry 3 --fail "https://raw.githubusercontent.com/wxglenovo/AdGuardHome-Filter/refs/heads/main/Create%20a%20Custom%20AdGuard%20Home%20Filtering%20Rule.txt" \
            | grep '^@@||' | sort | uniq > tmp/source_whitelist.txt

          # ✅ 最终顺序：wxglenovo 白名单 > 其他白名单 > 阻止规则
          cat tmp/source_whitelist.txt tmp/whitelist.txt tmp/blocklist.txt | sort | uniq > tmp/all_deduped.txt

          RAW_COUNT=$(wc -l < tmp/all_cleaned.txt)
          DEDUP_COUNT=$(wc -l < tmp/all_deduped.txt)
          DUP_COUNT=$((RAW_COUNT - DEDUP_COUNT))
          WHITELIST_COUNT=$(grep -c '^@@||' tmp/all_deduped.txt)
          TIMESTAMP=$(date '+%Y-%m-%d %H:%M:%S %Z')
          WHITE_RATIO=$(awk "BEGIN {printf \"%.2f\", (${WHITELIST_COUNT}/${DEDUP_COUNT})*100}")

          # 保存统计信息
          {
            echo "原始总数：$RAW_COUNT"
            echo "去重后：$DEDUP_COUNT"
            echo "重复行数：$DUP_COUNT"
            echo "白名单条数：$WHITELIST_COUNT"
            echo "白名单比例：${WHITE_RATIO}%"
            echo "更新时间：$TIMESTAMP"
            echo "来源列表：${#urls[@]} 个"
          } | tee dist/old_rules.txt

          # ✅ 生成 AdGuardHome.txt（带 ! 注释统计信息）
          {
            echo "! 更新时间：$TIMESTAMP"
            echo "! 原始规则数：$RAW_COUNT"
            echo "! 去重后规则数：$DEDUP_COUNT"
            echo "! 去重条数：$DUP_COUNT"
            echo "! 白名单条数：$WHITELIST_COUNT"
            echo "! 白名单比例：${WHITE_RATIO}%"
            echo "! 来源列表：${#urls[@]} 个"
            echo ""
            cat tmp/all_deduped.txt
          } > dist/AdGuardHome.txt

          echo "$DEDUP_COUNT" > dist/line_count.txt
