name: Build AdGuardHome rules

on:
  schedule:
    - cron: "0 0 * * *"
    - cron: "0 6 * * *"
    - cron: "0 12 * * *"
    - cron: "0 18 * * *"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download & Merge Rules (Clean + Deduplicate)
        run: |
          set -e
          mkdir -p dist tmp

          urls=(
            "https://raw.githubusercontent.com/wxglenovo/AdGuardHome-Filter/refs/heads/main/AdGuard%20Home_Allowlist.txt"
            "https://raw.githubusercontent.com/wxglenovo/AdGuardHome-Filter/refs/heads/main/AdGuard%20Home_Blacklist.txt"
            "https://gh-proxy.com/raw.githubusercontent.com/changzhaoCZ/fqnovel-adrules/refs/heads/main/fqnovel-fxxk_ads"
            "https://www.i-dont-care-about-cookies.eu/abp/"
            "https://anti-ad.net/easylist.txt"
            "https://raw.githubusercontent.com/REIJI007/Adblock-Rule-Collection/main/ADBLOCK_RULE_COLLECTION_DNS.txt"
            "https://raw.githubusercontent.com/217heidai/adblockfilters/main/rules/adblockdnslite.txt"
            "https://raw.githubusercontent.com/banbendalao/ADgk/master/ADgk.txt"
            "https://easylist-downloads.adblockplus.org/easyprivacy.txt"
            "https://easylist-downloads.adblockplus.org/easylistchina.txt"
            "https://easylist-downloads.adblockplus.org/easylist.txt"
            "https://raw.githubusercontent.com/BlueSkyXN/AdGuardHomeRules/master/skyrules.txt"
            "https://raw.githubusercontent.com/cjx82630/cjxlist/master/cjx-annoyance.txt"
            "https://raw.githubusercontent.com/hagezi/dns-blocklists/main/adblock/light.txt"
            "https://raw.githubusercontent.com/wxglenovo/Shadowrocket-to-AdGuard-Home/refs/heads/main/AdGuardHome.txt"
          )

          echo "📥 开始下载并合并所有规则..."
          rm -f tmp/all_raw.txt tmp/all_cleaned.txt tmp/all_deduped.txt
          for url in "${urls[@]}"; do
            echo "Downloading $url"
            curl -sSL --retry 3 --fail "$url" >> tmp/all_raw.txt || echo "Failed $url"
            echo "" >> tmp/all_raw.txt
          done

          echo "🧹 清理注释和空行..."
          grep -vE '^[[:space:]]*[!#]' tmp/all_raw.txt | sed '/^[[:space:]]*$/d' > tmp/all_cleaned.txt

          echo "📊 去重并排序 (LC_ALL=C sort -u)..."
          LC_ALL=C sort -u tmp/all_cleaned.txt > tmp/all_deduped.txt

          # 分离白名单和黑名单
          grep '^@@||' tmp/all_deduped.txt | LC_ALL=C sort -t'.' -k2,2 -k1,1 > dist/whitelist.txt || true
          grep -v '^@@||' tmp/all_deduped.txt | LC_ALL=C sort -t'.' -k2,2 -k1,1 > dist/blocklist.txt || true

          # 输出统计信息
          TIMESTAMP=$(TZ='Asia/Shanghai' date '+%Y-%m-%d %H:%M:%S %Z')
          RAW_COUNT=$(wc -l < tmp/all_cleaned.txt)
          DEDUP_COUNT=$(wc -l < tmp/all_deduped.txt)
          DUP_COUNT=$((RAW_COUNT - DEDUP_COUNT))
          WHITELIST_COUNT=$(wc -l < dist/whitelist.txt)
          BLACKLIST_COUNT=$(wc -l < dist/blocklist.txt)
          WHITE_RATIO=$(awk "BEGIN {printf \"%.2f\", (${WHITELIST_COUNT}/${DEDUP_COUNT})*100}")

          echo "原始总数：$RAW_COUNT" | tee dist/old_rules.txt
          echo "去重后：$DEDUP_COUNT" | tee -a dist/old_rules.txt
          echo "重复行数：$DUP_COUNT" | tee -a dist/old_rules.txt
          echo "白名单条数：$WHITELIST_COUNT" | tee -a dist/old_rules.txt
          echo "黑名单条数：$BLACKLIST_COUNT" | tee -a dist/old_rules.txt
          echo "白名单比例：${WHITE_RATIO}%" | tee -a dist/old_rules.txt
          echo "更新时间：$TIMESTAMP" | tee -a dist/old_rules.txt

          echo "✅ 文件生成完成，dist/目录文件列表:"
          ls -l dist/
          echo "前 10 行 whitelist:"
          head -n 10 dist/whitelist.txt
          echo "前 10 行 blocklist:"
          head -n 10 dist/blocklist.txt
