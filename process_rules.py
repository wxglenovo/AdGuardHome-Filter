import re
import requests
import os
from datetime import datetime, timedelta, timezone

# ===============================
# ğŸŒ ç™½åå•ä¸é»‘åå•åœ°å€
# ===============================
whitelist_url = 'https://raw.githubusercontent.com/wxglenovo/AdGuardHome-Filter/refs/heads/main/dist/whitelist.txt'
blocklist_url = 'https://raw.githubusercontent.com/wxglenovo/AdGuardHome-Filter/refs/heads/main/dist/blocklist.txt'

last_count_file = "last_count.txt"

# ===============================
# ğŸ“¥ è·å–è¿œç¨‹æ–‡ä»¶å¹¶æ¸…ç†æ— ç”¨è¡Œï¼ˆå»é™¤!å¼€å¤´çš„å¤´éƒ¨ä¿¡æ¯ï¼‰
# ===============================
def fetch_file(url):
    try:
        r = requests.get(url)
        r.raise_for_status()
        lines = []
        for line in r.text.splitlines():
            line = line.strip()
            if line and not line.startswith('!'):
                lines.append(line)
        return lines
    except requests.RequestException as e:
        print(f"âŒ è·å–æ–‡ä»¶å¤±è´¥: {e}")
        exit(1)

# è·å–ç™½åå•ä¸é»‘åå•è§„åˆ™
whitelist = fetch_file(whitelist_url)
blocklist = fetch_file(blocklist_url)

# ===============================
# ğŸ§© æå–ä¸»åŸŸå‡½æ•°
# ===============================
def get_base_domain(domain):
    parts = domain.split('.')
    if len(parts) >= 2:
        return '.'.join(parts[-2:])
    return domain

# ===============================
# âš™ï¸ è§„åˆ™æ¸…ç†å‡½æ•°ï¼ˆåŒ…æ‹¬åç¼€åŒ¹é… + æ—¥å¿—è¾“å‡ºï¼‰
# ===============================
def process_rules(rules, prefix, list_name):
    cleaned = []
    keep_dict = {}  # è®°å½•çˆ¶åŸŸ + åç¼€
    parsed_rules = []

    # é¢„è§£æè§„åˆ™
    for line in rules:
        if not line.startswith(prefix):
            continue
        body = line[len(prefix):]
        match = re.match(r"([^/^\$]+)([\/\^\$].*)?$", body)
        if not match:
            continue
        domain = match.group(1)
        suffix = match.group(2) if match.group(2) else ""
        parsed_rules.append((line, domain, suffix))

    deleted_count = 0

    print(f"\nğŸ§¹ æ­£åœ¨å¤„ç† {list_name}...ï¼ˆå…± {len(parsed_rules)} æ¡è§„åˆ™ï¼‰")

    for line, domain, suffix in parsed_rules:
        base = get_base_domain(domain)
        key = (base, suffix)

        if key not in keep_dict:
            keep_dict[key] = line
            cleaned.append(line)
        else:
            # æ£€æŸ¥æ˜¯å¦ä¸ºå­åŸŸï¼ˆå¦‚ a.example.com å±äº example.comï¼‰
            if domain.endswith(base) and domain != base:
                deleted_count += 1
                print(f"ğŸ—‘ï¸ åŒ¹é…åˆ é™¤: {line}  â†’ ä¿ç•™çˆ¶åŸŸ: {keep_dict[key]}")
                continue
            else:
                cleaned.append(line)

    print(f"âœ… {list_name} æ¸…ç†å®Œæˆï¼šå…±åˆ é™¤ {deleted_count} æ¡\n")
    return cleaned, deleted_count

# ===============================
# ğŸ§¹ åˆ†åˆ«å¤„ç†ç™½åå•ä¸é»‘åå•
# ===============================
cleaned_whitelist, deleted_whitelist = process_rules(whitelist, "@@||", "ç™½åå•")
cleaned_blocklist, deleted_blocklist = process_rules(blocklist, "||", "é»‘åå•")

# ===============================
# ğŸ“Š è¯»å–ä¸ä¿å­˜ä¸Šæ¬¡ç»Ÿè®¡æ•°é‡
# ===============================
def read_last_count():
    if os.path.exists(last_count_file):
        with open(last_count_file, 'r') as f:
            lines = f.read().splitlines()
            if len(lines) >= 2:
                return int(lines[0]), int(lines[1])
    return 0, 0

def write_current_count(w_count, b_count):
    with open(last_count_file, 'w') as f:
        f.write(f"{w_count}\n{b_count}\n")

current_w = len(cleaned_whitelist)
current_b = len(cleaned_blocklist)
last_w, last_b = read_last_count()
diff_w = current_w - last_w
diff_b = current_b - last_b

# ===============================
# ğŸ§¾ ç”Ÿæˆå¤´éƒ¨ä¿¡æ¯ï¼ˆç‹¬ç«‹æ˜¾ç¤ºï¼‰
# ===============================
def generate_header(list_type, url, original_count, deleted_count, current_count, diff_count):
    beijing_time = datetime.now(timezone(timedelta(hours=8))).strftime('%Y-%m-%d %H:%M:%S')

    diff_str = (
        f"å¢åŠ  {diff_count} æ¡" if diff_count > 0 else
        f"å‡å°‘ {abs(diff_count)} æ¡" if diff_count < 0 else "æ— å˜åŒ– 0 æ¡"
    )

    header = f"""###########################################################
# ğŸ“… AdGuardHome {list_type} è‡ªåŠ¨æ„å»ºä¿¡æ¯
# â° æ›´æ–°æ—¶é—´: {beijing_time} CST
# ğŸŒ è§„åˆ™æ¥æº: {url}
# --------------------------------------------------------
# ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
#   â–¸ åŸå§‹è§„åˆ™æ•°é‡: {original_count}
#   â–¸ åˆ é™¤å­åŸŸæ•°é‡: {deleted_count}
#   â–¸ æ¸…ç†åè§„åˆ™æ•°é‡: {current_count}
#   â–¸ ä¸ä¸Šæ¬¡å¯¹æ¯”: {diff_str}
# --------------------------------------------------------
# ğŸ§© è¯´æ˜:
#   å½“çˆ¶åŸŸä¸å­åŸŸï¼ˆåŒ…æ‹¬ç›¸åŒåç¼€ï¼‰åŒæ—¶å­˜åœ¨æ—¶ï¼Œä¿ç•™çˆ¶åŸŸè§„åˆ™ã€‚
#   ä¾‹å¦‚ï¼š||beyondthewords.co.uk^ ä¸ ||a.beyondthewords.co.uk^ â†’ ä¿ç•™å‰è€…ã€‚
# ==========================================================
"""
    return header

header_whitelist = generate_header(
    "ç™½åå•", whitelist_url, len(whitelist), deleted_whitelist, len(cleaned_whitelist), diff_w
)
header_blocklist = generate_header(
    "é»‘åå•", blocklist_url, len(blocklist), deleted_blocklist, len(cleaned_blocklist), diff_b
)

# ===============================
# ğŸ’¾ è¾“å‡ºä¸ºä¸¤ä¸ªæ–‡ä»¶
# ===============================
with open("cleaned_whitelist.txt", "w", encoding="utf-8") as f:
    f.write(header_whitelist + "\n")
    f.write("\n".join(sorted(cleaned_whitelist)) + "\n")

with open("cleaned_blocklist.txt", "w", encoding="utf-8") as f:
    f.write(header_blocklist + "\n")
    f.write("\n".join(sorted(cleaned_blocklist)) + "\n")

write_current_count(len(cleaned_whitelist), len(cleaned_blocklist))

# ===============================
# âœ… æ§åˆ¶å°è¾“å‡ºæ‘˜è¦
# ===============================
print("âœ… æ„å»ºå®Œæˆï¼")
print(f"ç™½åå•æ¸…ç†å: {len(cleaned_whitelist)} æ¡ï¼ˆåˆ é™¤ {deleted_whitelist} æ¡ï¼‰")
print(f"é»‘åå•æ¸…ç†å: {len(cleaned_blocklist)} æ¡ï¼ˆåˆ é™¤ {deleted_blocklist} æ¡ï¼‰")
print("è¾“å‡ºæ–‡ä»¶: cleaned_whitelist.txt, cleaned_blocklist.txt")
