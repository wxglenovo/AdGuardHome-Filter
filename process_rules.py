
import re
import requests
from datetime import datetime
from pathlib import Path

# ==========================================================
# ğŸ“Œ è§„åˆ™æ¥æº
# ==========================================================
whitelist_url = "https://raw.githubusercontent.com/wxglenovo/AdGuardHome-Filter/refs/heads/main/dist/whitelist.txt"
blocklist_url = "https://raw.githubusercontent.com/wxglenovo/AdGuardHome-Filter/refs/heads/main/dist/blocklist.txt"

# ==========================================================
# ğŸ“Œ ä¸‹è½½è§„åˆ™
# ==========================================================
def download_rules(url):
    print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½è§„åˆ™æ–‡ä»¶: {url}")
    resp = requests.get(url, timeout=60)
    resp.encoding = "utf-8"
    lines = [line.strip() for line in resp.text.splitlines() if line.strip()]
    # åˆ é™¤ä»¥ "!" å¼€å¤´çš„æ—§å¤´éƒ¨ä¿¡æ¯
    lines = [l for l in lines if not l.startswith("!")]
    return lines

# ==========================================================
# ğŸ“Œ æå–åŸŸåä¸è§„åˆ™åç¼€
# ==========================================================
def extract_domain_and_suffix(rule):
    match = re.match(r"(@@)?\|\|([^/^$]+)(.*)", rule)
    if match:
        prefix = match.group(1) or ""
        domain = match.group(2).strip(".")
        suffix = match.group(3)
        return prefix, domain, suffix
    return "", "", ""

# ==========================================================
# ğŸ“Œ åˆ¤æ–­æ˜¯å¦ä¸ºå­åŸŸ
# ==========================================================
def is_subdomain(child, parent):
    return child.endswith("." + parent)

# ==========================================================
# ğŸ“Œ æ¸…ç†é€»è¾‘ï¼šåˆ é™¤çˆ¶åŸŸ + å­åŸŸé‡å¤é¡¹ï¼ˆåç¼€å®Œå…¨ä¸€è‡´ï¼‰
# ==========================================================
def clean_rules(rules):
    parsed = []
    for rule in rules:
        prefix, domain, suffix = extract_domain_and_suffix(rule)
        if domain:
            parsed.append((prefix, domain, suffix, rule))

    parsed.sort(key=lambda x: x[1].count("."))
    skip = set()

    for i, (prefix_i, domain_i, suffix_i, rule_i) in enumerate(parsed):
        if rule_i in skip:
            continue
        for j, (prefix_j, domain_j, suffix_j, rule_j) in enumerate(parsed):
            if i != j and rule_j not in skip:
                # å½“çˆ¶åŸŸ + å­åŸŸåç¼€ä¸€è‡´ï¼Œåˆ é™¤å­åŸŸè§„åˆ™
                if (
                    prefix_i == prefix_j
                    and suffix_i == suffix_j
                    and is_subdomain(domain_j, domain_i)
                ):
                    skip.add(rule_j)

    cleaned = [rule for _, _, _, rule in parsed if rule not in skip]
    return cleaned, len(skip)

# ==========================================================
# ğŸ“Œ ç”Ÿæˆå¤´éƒ¨ä¿¡æ¯
# ==========================================================
def build_header(stats):
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S CST")
    header = f"""###########################################################
# ğŸ“… AdGuardHome ç»¼åˆè§„åˆ™è‡ªåŠ¨æ„å»ºä¿¡æ¯
# â° æ›´æ–°æ—¶é—´: {now}
# ğŸŒ è§„åˆ™æ¥æº:
#   ç™½åå•: {whitelist_url}
#   é»‘åå•: {blocklist_url}
# --------------------------------------------------------
# ç™½åå•åŸå§‹è§„åˆ™æ•°é‡: {stats['white_total']}
# ç™½åå•åˆ é™¤å­åŸŸæ•°é‡: {stats['white_removed']}
# ç™½åå•æ¸…ç†åè§„åˆ™æ•°é‡: {stats['white_final']}
# --------------------------------------------------------
# é»‘åå•åŸå§‹è§„åˆ™æ•°é‡: {stats['black_total']}
# é»‘åå•åˆ é™¤å­åŸŸæ•°é‡: {stats['black_removed']}
# é»‘åå•æ¸…ç†åè§„åˆ™æ•°é‡: {stats['black_final']}
# --------------------------------------------------------
# è¯´æ˜:
#   å½“çˆ¶åŸŸä¸å­åŸŸï¼ˆåŒ…æ‹¬è§„åˆ™åç¼€ï¼‰åŒæ—¶å­˜åœ¨æ—¶ï¼Œä¿ç•™çˆ¶åŸŸè§„åˆ™ï¼Œåˆ é™¤å­åŸŸè§„åˆ™ã€‚
#   å¤šçº§å­åŸŸï¼ˆä¸‰çº§ã€å››çº§ï¼‰åˆ™ä¿ç•™çº§æ•°æ›´ä½çš„åŸŸåï¼ˆçˆ¶åŸŸï¼‰ã€‚
# ==========================================================
"""
    return header

# ==========================================================
# ğŸ“Œ ä¸»æ‰§è¡Œé€»è¾‘
# ==========================================================
def main():
    print("ğŸš€ å¼€å§‹å¤„ç† AdGuardHome è§„åˆ™...")

    whitelist = download_rules(whitelist_url)
    blocklist = download_rules(blocklist_url)

    print("ğŸ§¹ æ¸…ç†ç™½åå•ä¸­å­åŸŸè§„åˆ™...")
    cleaned_white, removed_white = clean_rules(whitelist)

    print("ğŸ§¹ æ¸…ç†é»‘åå•ä¸­å­åŸŸè§„åˆ™...")
    cleaned_black, removed_black = clean_rules(blocklist)

    stats = {
        "white_total": len(whitelist),
        "white_removed": removed_white,
        "white_final": len(cleaned_white),
        "black_total": len(blocklist),
        "black_removed": removed_black,
        "black_final": len(cleaned_black),
    }

    header = build_header(stats)
    all_rules = cleaned_white + cleaned_black
    output_text = header + "\n".join(all_rules) + "\n"

    Path("dist").mkdir(exist_ok=True)
    output_path = Path("dist/AdGuardHome_Filter.txt")

    with open(output_path, "w", encoding="utf-8") as f:
        f.write(output_text)

    print(f"âœ… è§„åˆ™å·²ç”Ÿæˆ â†’ {output_path}")
    print(f"ğŸ“Š ç™½åå•æ¸…ç† {removed_white} æ¡ï¼Œé»‘åå•æ¸…ç† {removed_black} æ¡ã€‚")

# ==========================================================
if __name__ == "__main__":
    main()
